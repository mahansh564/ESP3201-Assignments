{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"7LDzqMh0-rKh"},"outputs":[],"source":["import random\n","import sys\n","import mdp\n","import environment\n","import util\n","import optparse\n","\n","class Gridworld(mdp.MarkovDecisionProcess):\n","  \"\"\"\n","    Gridworld\n","  \"\"\"\n","  def __init__(self, grid):\n","    # layout\n","    if type(grid) == type([]): grid = makeGrid(grid)\n","    self.grid = grid\n","\n","    # parameters\n","    self.livingReward = -1 # 0.0\n","    self.noise = 0.2\n","\n","  def setLivingReward(self, reward):\n","    \"\"\"\n","    The (negative) reward for exiting \"normal\" states.\n","\n","    Note that in the R+N text, this reward is on entering\n","    a state and therefore is not clearly part of the state's\n","    future rewards.\n","    \"\"\"\n","    self.livingReward = reward\n","\n","  def setNoise(self, noise):\n","    \"\"\"\n","    The probability of moving in an unintended direction.\n","    \"\"\"\n","    self.noise = noise\n","\n","\n","  def getPossibleActions(self, state):\n","    \"\"\"\n","    Returns list of valid actions for 'state'.\n","\n","    Note that you can request moves into walls and\n","    that \"exit\" states transition to the terminal\n","    state under the special action \"done\".\n","    \"\"\"\n","    if state == self.grid.terminalState:\n","      return ()\n","    x,y = state\n","    if type(self.grid[x][y]) == int:\n","      return ('exit',)\n","    return ('north','west','south','east')\n","\n","  def getStates(self):\n","    \"\"\"\n","    Return list of all states.\n","    \"\"\"\n","    # The true terminal state.\n","    states = [self.grid.terminalState]\n","    for x in range(self.grid.width):\n","      for y in range(self.grid.height):\n","        if self.grid[x][y] != '#':\n","          state = (x,y)\n","          states.append(state)\n","    return states\n","\n","  def getReward(self, state, action, nextState):\n","    \"\"\"\n","    Get reward for state, action, nextState transition.\n","\n","    Note that the reward depends only on the state being\n","    departed (as in the R+N book examples, which more or\n","    less use this convention).\n","    \"\"\"\n","    if state == self.grid.terminalState:\n","      return 0.0\n","    x, y = state\n","    cell = self.grid[x][y]\n","    if type(cell) == int or type(cell) == float:\n","      return cell\n","    return self.livingReward\n","\n","  def getStartState(self):\n","    for x in range(self.grid.width):\n","      for y in range(self.grid.height):\n","        if self.grid[x][y] == 'S':\n","          return (x, y)\n","    raise 'Grid has no start state'\n","\n","  def isTerminal(self, state):\n","    \"\"\"\n","    Only the TERMINAL_STATE state is *actually* a terminal state.\n","    The other \"exit\" states are technically non-terminals with\n","    a single action \"exit\" which leads to the true terminal state.\n","    This convention is to make the grids line up with the examples\n","    in the R+N textbook.\n","    \"\"\"\n","    return state == self.grid.terminalState\n","\n","\n","  def getTransitionStatesAndProbs(self, state, action):\n","    \"\"\"\n","    Returns list of (nextState, prob) pairs\n","    representing the states reachable\n","    from 'state' by taking 'action' along\n","    with their transition probabilities.\n","    \"\"\"\n","\n","    if action not in self.getPossibleActions(state):\n","      raise \"Illegal action!\"\n","\n","    if self.isTerminal(state):\n","      return []\n","\n","    x, y = state\n","\n","    if type(self.grid[x][y]) == int or type(self.grid[x][y]) == float:\n","      termState = self.grid.terminalState\n","      return [(termState, 1.0)]\n","\n","    successors = []\n","\n","    northState = (self.__isAllowed(y+1,x) and (x,y+1)) or state\n","    westState = (self.__isAllowed(y,x-1) and (x-1,y)) or state\n","    southState = (self.__isAllowed(y-1,x) and (x,y-1)) or state\n","    eastState = (self.__isAllowed(y,x+1) and (x+1,y)) or state\n","\n","    if action == 'north' or action == 'south':\n","      if action == 'north':\n","        successors.append((northState,1-self.noise))\n","      else:\n","        successors.append((southState,1-self.noise))\n","\n","      massLeft = self.noise\n","      successors.append((westState,massLeft/2.0))\n","      successors.append((eastState,massLeft/2.0))\n","\n","    if action == 'west' or action == 'east':\n","      if action == 'west':\n","        successors.append((westState,1-self.noise))\n","      else:\n","        successors.append((eastState,1-self.noise))\n","\n","      massLeft = self.noise\n","      successors.append((northState,massLeft/2.0))\n","      successors.append((southState,massLeft/2.0))\n","\n","    successors = self.__aggregate(successors)\n","\n","    return successors\n","\n","  def __aggregate(self, statesAndProbs):\n","    counter = util.Counter()\n","    for state, prob in statesAndProbs:\n","      counter[state] += prob\n","    newStatesAndProbs = []\n","    for state, prob in counter.items():\n","      newStatesAndProbs.append((state, prob))\n","    return newStatesAndProbs\n","\n","  def __isAllowed(self, y, x):\n","    if y < 0 or y >= self.grid.height: return False\n","    if x < 0 or x >= self.grid.width: return False\n","    return self.grid[x][y] != '#'\n","\n","class GridworldEnvironment(environment.Environment):\n","\n","  def __init__(self, gridWorld):\n","    self.gridWorld = gridWorld\n","    self.reset()\n","\n","  def getCurrentState(self):\n","    return self.state\n","\n","  def getPossibleActions(self, state):\n","    return self.gridWorld.getPossibleActions(state)\n","\n","  def doAction(self, action):\n","    successors = self.gridWorld.getTransitionStatesAndProbs(self.state, action)\n","    sum = 0.0\n","    rand = random.random()\n","    state = self.getCurrentState()\n","    for nextState, prob in successors:\n","      sum += prob\n","      if sum > 1.0:\n","        raise 'Total transition probability more than one; sample failure.'\n","      if rand < sum:\n","        reward = self.gridWorld.getReward(state, action, nextState)\n","        self.state = nextState\n","        return (nextState, reward)\n","    raise 'Total transition probability less than one; sample failure.'\n","\n","  def reset(self):\n","    self.state = self.gridWorld.getStartState()\n","\n","class Grid:\n","  \"\"\"\n","  A 2-dimensional array of immutables backed by a list of lists.  Data is accessed\n","  via grid[x][y] where (x,y) are cartesian coordinates with x horizontal,\n","  y vertical and the origin (0,0) in the bottom left corner.\n","\n","  The __str__ method constructs an output that is oriented appropriately.\n","  \"\"\"\n","  def __init__(self, width, height, initialValue=' '):\n","    self.width = width\n","    self.height = height\n","    self.data = [[initialValue for y in range(height)] for x in range(width)]\n","    self.terminalState = 'TERMINAL_STATE'\n","\n","  def __getitem__(self, i):\n","    return self.data[i]\n","\n","  def __setitem__(self, key, item):\n","    self.data[key] = item\n","\n","  def __eq__(self, other):\n","    if other == None: return False\n","    return self.data == other.data\n","\n","  def __hash__(self):\n","    return hash(self.data)\n","\n","  def copy(self):\n","    g = Grid(self.width, self.height)\n","    g.data = [x[:] for x in self.data]\n","    return g\n","\n","  def deepCopy(self):\n","    return self.copy()\n","\n","  def shallowCopy(self):\n","    g = Grid(self.width, self.height)\n","    g.data = self.data\n","    return g\n","\n","  def _getLegacyText(self):\n","    t = [[self.data[x][y] for x in range(self.width)] for y in range(self.height)]\n","    t.reverse()\n","    return t\n","\n","  def __str__(self):\n","    return str(self._getLegacyText())\n","\n","def makeGrid(gridString):\n","  width, height = len(gridString[0]), len(gridString)\n","  grid = Grid(width, height)\n","  for ybar, line in enumerate(gridString):\n","    y = height - ybar - 1\n","    for x, el in enumerate(line):\n","      grid[x][y] = el\n","  return grid\n","\n","\n","def getCliffGrid():\n","  grid = [[' ', ' ', ' ', ' '],\n","          [' ', ' ', ' ', ' '],\n","          [' ', ' ', ' ', ' '],\n","          ['S', -100,-100, 10]]\n","  return Gridworld(grid)\n","\n","def getCliffGrid2():\n","  grid = [[' ', ' ', ' ', ' ', ' ', ' '],\n","          [' ', ' ', ' ', ' ', ' ', ' '],\n","          [' ', ' ', ' ', ' ', ' ', ' '],\n","          ['S', -100,-100, -100,-100, 10]]\n","  return Gridworld(grid)\n","\n","def getBookGrid():\n","  grid = [[' ',' ',' ',+1],\n","          [' ','#',' ',-1],\n","          ['S',' ',' ',' ']]\n","  return Gridworld(grid)\n","\n","\n","def getUserAction(state, actionFunction):\n","  \"\"\"\n","  Get an action from the user (rather than the agent).\n","\n","  Used for debugging and lecture demos.\n","  \"\"\"\n","  action = None\n","  while True:\n","    keys = input('Move with <w,s,a,d> + Enter. Press q + Enter to quit ')\n","    if 'w' in keys: action = 'north'\n","    if 's' in keys: action = 'south'\n","    if 'a' in keys: action = 'west'\n","    if 'd' in keys: action = 'east'\n","    if 'q' in keys: sys.exit(0)\n","    if action == None: continue\n","    break\n","  actions = actionFunction(state)\n","  if action not in actions:\n","    action = actions[0]\n","  return action\n","\n","def printString(x): print(x)\n","\n","def runEpisode(agent, environment, discount, decision, display, message, pause, episode):\n","  returns = 0\n","  totalDiscount = 1.0\n","  environment.reset()\n","\n","  if 'startEpisode' in dir(agent): agent.startEpisode()\n","  message(\"BEGINNING EPISODE: \"+str(episode)+\"\\n\")\n","  while True:\n","    # DISPLAY CURRENT STATE\n","    state = environment.getCurrentState()\n","    display(state)\n","    pause()\n","\n","    # END IF IN A TERMINAL STATE\n","    actions = environment.getPossibleActions(state)\n","    if len(actions) == 0:\n","      message(\"EPISODE \"+str(episode)+\" COMPLETE: RETURN WAS \"+str(returns)+\"\\n\")\n","      return returns\n","\n","    # GET ACTION (USUALLY FROM AGENT)\n","    action = decision(state)\n","    if action == None:\n","      raise 'Error: Agent returned None action'\n","\n","    # EXECUTE ACTION\n","    nextState, reward = environment.doAction(action)\n","    message(\"Started in state: \"+str(state)+\n","            \"\\nTook action: \"+str(action)+\n","            \"\\nEnded in state: \"+str(nextState)+\n","            \"\\nGot reward: \"+str(reward)+\"\\n\")\n","    # UPDATE LEARNER\n","    if 'observeTransition' in dir(agent):\n","        agent.observeTransition(state, action, nextState, reward)\n","\n","    returns += reward * totalDiscount\n","    totalDiscount *= discount\n","\n","  if 'stopEpisode' in dir(agent):\n","    agent.stopEpisode()\n","\n","def parseOptions():\n","    optParser = optparse.OptionParser()\n","    optParser.add_option('-d', '--discount',action='store',\n","                         type='float',dest='discount',default=0.9,\n","                         help='Discount on future (default %default)')\n","    optParser.add_option('-r', '--livingReward',action='store',\n","                         type='float',dest='livingReward',default=-1.0,\n","                         metavar=\"R\", help='Reward for living for a time step (default %default)')\n","    optParser.add_option('-n', '--noise',action='store',\n","                         type='float',dest='noise',default=0.0,\n","                         metavar=\"P\", help='How often action results in ' +\n","                         'unintended direction (default %default)' )\n","    optParser.add_option('-e', '--epsilon',action='store',\n","                         type='float',dest='epsilon',default=0.3,\n","                         metavar=\"E\", help='Chance of taking a random action in q-learning (default %default)')\n","    optParser.add_option('-l', '--learningRate',action='store',\n","                         type='float',dest='learningRate',default=0.5,\n","                         metavar=\"P\", help='TD learning rate (default %default)' )\n","    optParser.add_option('-i', '--iterations',action='store',\n","                         type='int',dest='iters',default=10,\n","                         metavar=\"K\", help='Number of rounds of value iteration (default %default)')\n","    optParser.add_option('-k', '--episodes',action='store',\n","                         type='int',dest='episodes',default=1,\n","                         metavar=\"K\", help='Number of epsiodes of the MDP to run (default %default)')\n","    optParser.add_option('-g', '--grid',action='store',\n","                         metavar=\"G\", type='string',dest='grid',default=\"CliffGrid\",\n","                         help='Grid to use (case sensitive; options are BookGrid, CliffGrid' )\n","    optParser.add_option('-w', '--windowSize', metavar=\"X\", type='int',dest='gridSize',default=150,\n","                         help='Request a window width of X pixels *per grid cell* (default %default)')\n","    optParser.add_option('-a', '--agent',action='store', metavar=\"A\",\n","                         type='string',dest='agent',default=\"random\",\n","                         help='Agent type (options are \\'random\\', \\'value\\' and \\'q\\', default %default)')\n","    optParser.add_option('-p', '--pause',action='store_true',\n","                         dest='pause',default=False,\n","                         help='Pause GUI after each time step when running the MDP')\n","    optParser.add_option('-q', '--quiet',action='store_true',\n","                         dest='quiet',default=False,\n","                         help='Skip display of any learning episodes')\n","    optParser.add_option('-s', '--speed',action='store', metavar=\"S\", type=float,\n","                         dest='speed',default=1.0,\n","                         help='Speed of animation, S > 1.0 is faster, 0.0 < S < 1.0 is slower (default %default)')\n","    optParser.add_option('-m', '--manual',action='store_true',\n","                         dest='manual',default=False,\n","                         help='Manually control agent')\n","    optParser.add_option('-v', '--valueSteps',action='store_true' ,default=False,\n","                         help='Display each step of value iteration')\n","\n","    opts, args = optParser.parse_args()\n","\n","    if opts.manual and opts.agent != 'q':\n","      print('## Disabling Agents in Manual Mode (-m) ##')\n","      opts.agent = None\n","\n","    # MANAGE CONFLICTS\n","    if opts.quiet:\n","      opts.pause = False\n","\n","    if opts.manual:\n","      opts.pause = True\n","\n","    return opts"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wv1EsB5L-rKl"},"outputs":[],"source":["import gridworld\n","\n","# opts\n","grid = 'CliffGrid'\n","livingReward = -1\n","noise = 0.0\n","\n","mdpFunction = getattr(gridworld, \"get\"+ grid)\n","mdp = mdpFunction()\n","mdp.setLivingReward(livingReward)\n","mdp.setNoise(noise)\n","env = gridworld.GridworldEnvironment(mdp)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hnmBAzRM-rKl"},"outputs":[],"source":["# ###########################\n","# # GET THE DISPLAY ADAPTER\n","# ###########################\n","\n","import textGridworldDisplay\n","display = textGridworldDisplay.TextGridworldDisplay(mdp)\n","display.start()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wAVEcI7g-rKm","outputId":"b06a9ee2-cfe1-4fe9-d44a-306c00e6ae28"},"outputs":[{"name":"stdout","output_type":"stream","text":["--  -------  -------  -----\n","\n","\n","\n","S*  -100.00  -100.00  10.00\n","--  -------  -------  -----\n"]}],"source":["# display\n","displayCallback = lambda state: display.displayNullValues(state)\n","\n","state = env.getCurrentState()\n","displayCallback(state)"]},{"cell_type":"markdown","metadata":{"id":"plHop45d-rKn"},"source":["## Value Iteration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6v5fTOsz-rKo"},"outputs":[],"source":["'''\n","class ValueIterationAgent(ValueEstimationAgent):\n","\n","  def __init__(self, mdp, discount = 0.9, iterations = 100):\n","        ...\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SE0F6azY-rKo","outputId":"92ce1e13-bf0b-4ab7-a2b7-51c472c3c6d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["['TERMINAL_STATE', (0, 0), (0, 1), (0, 2), (0, 3), (1, 0), (1, 1), (1, 2), (1, 3), (2, 0), (2, 1), (2, 2), (2, 3), (3, 0), (3, 1), (3, 2), (3, 3)]\n"]}],"source":["# States, S\n","states = mdp.getStates()\n","print(states)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iA6JDsJK-rKo","outputId":"eef4835f-9252-4aa6-bd5f-e27e26d1958e"},"outputs":[{"name":"stdout","output_type":"stream","text":["('north', 'west', 'south', 'east')\n"]}],"source":["i_s = 1 # state idx\n","i_a = 0 # action idx\n","\n","# Actions, A, in 1 particular state, s\n","actions = mdp.getPossibleActions(states[i_s])\n","print(actions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3YXVYD3G-rKp","outputId":"42144174-8e46-4746-f92d-c6ac6acfb614"},"outputs":[{"name":"stdout","output_type":"stream","text":["[((0, 1), 1.0), ((0, 0), 0.0), ((1, 0), 0.0)]\n"]}],"source":["# P(.|s,a)\n","transitions = mdp.getTransitionStatesAndProbs(states[i_s], actions[i_a])\n","print(transitions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bZwY_Gqi-rKp","outputId":"0d8e5d1d-738b-4c28-f2cc-da2a497ed7a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["-1\n"]}],"source":["# reward a\n","reward = mdp.getReward(states[i_s], actions[i_a], transitions[0][0]) # state, action, nextState\n","print(reward)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OD746HYh-rKp"},"outputs":[],"source":["decisionCallback = a.getAction\n","\n","'''\n","def runEpisode(agent, environment, discount, decision, display, message, pause, episode):\n","  returns = 0\n","  totalDiscount = 1.0\n","  environment.reset()\n","\n","  if 'startEpisode' in dir(agent): agent.startEpisode()\n","  message(\"BEGINNING EPISODE: \"+str(episode)+\"\\n\")\n","  while True:\n","    # DISPLAY CURRENT STATE\n","    state = environment.getCurrentState()\n","    display(state)\n","    pause()\n","\n","    # END IF IN A TERMINAL STATE\n","    actions = environment.getPossibleActions(state)\n","    if len(actions) == 0:\n","      message(\"EPISODE \"+str(episode)+\" COMPLETE: RETURN WAS \"+str(returns)+\"\\n\")\n","      return returns\n","\n","    # GET ACTION (USUALLY FROM AGENT)\n","    action = decision(state)\n","\n","    if action == None:\n","      raise 'Error: Agent returned None action'\n","\n","    # EXECUTE ACTION\n","    nextState, reward = environment.doAction(action)\n","    message(\"Started in state: \"+str(state)+\n","            \"\\nTook action: \"+str(action)+\n","            \"\\nEnded in state: \"+str(nextState)+\n","            \"\\nGot reward: \"+str(reward)+\"\\n\")\n","    # UPDATE LEARNER\n","    if 'observeTransition' in dir(agent):\n","        agent.observeTransition(state, action, nextState, reward)\n","\n","    returns += reward * totalDiscount\n","    totalDiscount *= discount\n","\n","  if 'stopEpisode' in dir(agent):\n","    agent.stopEpisode()\n","\n","'''"]},{"cell_type":"markdown","metadata":{"id":"Ij6cXTOG-rKq"},"source":["## Q-Learning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O5q978XJ-rKq","outputId":"c7045de9-6dd0-49b3-e4ae-642bca122fa3"},"outputs":[{"data":{"text/plain":["'\\nclass QLearningAgent(ReinforcementAgent):\\n    def __init__(self, **args):\\n    \"You can initialize Q-values here...\"\\n    ReinforcementAgent.__init__(self, **args)\\n\\n    self.epsilon = args[\\'epsilon\\']\\n    self.alpha = args[\\'alpha\\']\\n    self.gamma = args[\\'gamma\\']\\n    self.actionFn = args[\\'actionFn\\']\\n'"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","class QLearningAgent(ReinforcementAgent):\n","    def __init__(self, **args):\n","    \"You can initialize Q-values here...\"\n","    ReinforcementAgent.__init__(self, **args)\n","\n","    self.epsilon = args['epsilon']\n","    self.alpha = args['alpha']\n","    self.gamma = args['gamma']\n","    self.actionFn = args['actionFn']\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i5QwOYfb-rKq"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}